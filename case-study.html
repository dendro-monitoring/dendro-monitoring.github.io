<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">

<head>
  <meta charset="utf-8" />
  <title>Dendro • Case Study</title>
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:regular,500,600,700" media="all" />
  <script type="text/javascript">
    WebFont.load({ google: { families: ["Inter:regular,500,600,700"] } });
  </script>
  <script type="text/javascript">
    !(function (o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      (n.className += t + "js"),
        ("ontouchstart" in o ||
          (o.DocumentTouch && c instanceof DocumentTouch)) &&
        (n.className += t + "touch");
    })(window, document);
  </script>
  <link href="assets/images/logo-mono.png" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/images/logo-mono.png" rel="apple-touch-icon" />
  <script src="https://kit.fontawesome.com/d019875f94.js" crossorigin="anonymous"></script>
  <meta name="image" property="og:image" content="assets/images/thumbnail.png" />
</head>

<body>
  <div class="navigation-wrap">
    <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
      <div class="navigation-container">
        <div class="navigation-left">
          <a href="/" aria-current="page" class="brand w-nav-brand w—current" aria-label="home">
            <img src="assets/images/logo-mono.png" alt="" class="template-logo" />
          </a>
          <nav role="navigation" class="nav-menu w-nav-menu">
            <a href="/case-study" class="link-block w-inline-block">
              <div>Case Study</div>
            </a>
            <a href="/team" class="link-block w-inline-block">
              <div>The Team</div>
            </a>
          </nav>
        </div>
        <div class="navigation-right">
          <div class="login-buttons">
            <a href="https://github.com/dendro-monitoring" target="_blank">
              <span style="color: #161d6f">
                <i class="fab fa-github fa-lg"></i>
              </span>
            </a>
          </div>
        </div>
      </div>
      <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
    </div>
  </div>
  <div id="sidebar" class="toc">
  </div>
  <div class="section header">
    <article class="container case-study-container">
      <div class="hero-text-container">
        <h1 class="h1 centered">Case Study</h1>
      </div>
      <div id="case-study">

        <br />
        <br />

        <!-- Section 1 -->
        <h2 class="h2">1 Introduction</h2>
        <br>
        <p>
          The last decade has seen a paradigm shift in computing infrastructure. Systems are becoming more and more
          ephemeral, abstracted, and distributed. Innovations in computing infrastructure and software architectures
          have yielded simpler components at higher levels of abstraction, but behind those simpler pieces lurk more
          complex systems. Engineers are able to deploy and stitch together many more different kinds of software
          components today, resulting in vastly increased complexity. As individual nodes in your system’s topology
          become more decentralized, they also become more complicated to track and monitor.
        </p>

        <h3>1.1 What is Dendro?</h3>
        <p>
          Dendro is an open-source, serverless monitoring framework for small, distributed apps. Our team built Dendro
          in order to help small teams handle the challenges of operating a distributed system.
        </p>
        <br />
        <p>
          Dendro enables you to collect, centralize, and store log and metric data emitted by the various disparate
          pieces of your system. With Dendro, when something goes down in production, there’s no race to SSH into a
          dozen different nodes to figure out what’s gone wrong; all of the log data you’d use for debugging has already
          been collected, processed and tagged with the servers and services that originated the individual records, and
          stored in a single database for querying.
        </p>
        <br />
        <p>
          All of this empowers the user to both greatly reduce costly downtime or outages, and proactively improve the
          overall performance of their systems. Prevention is important: you want to catch problems before your users
          do. If you’re constantly applying ad-hoc, band-aid fixes, you’re making your system more fragile instead of
          less.
        </p>
        <br />
        <p>
          To illustrate just exactly what Dendro does, let’s use the example of a small company named NapTime.
        </p>
        <br />

        <h3>1.2 NapTime: An Example User Story</h3>
        <p>
          NapTime has two engineers, and they recently completed the beta build of their sleep monitoring app.
        </p>
        <br />
        <p>
          Their system itself has proven to be resilient so far, thanks to its small size and simplicity.
        </p>
        <figure>
          <img src="assets/images/case-study/naptime-early.png" class="case-study-image" />
          <figcaption>Fig. 1: NapTime's beta architecture</figcaption>
        </figure>
        <br />
        <p>
          NapTime built a simple three-tier system in order to avoid premature optimization, with their web server and
          application running on one server, and their database on another.
        </p>
        <br />
        <p>
          The two engineers have been busy developing features and shipping their first few builds. When an outage or
          critical bug pops up, it derails their development flow. NapTime doesn’t currently have a monitoring solution
          in place, so they’re totally reliant on either hearing from users when something breaks, or coming across an
          issue themselves by poking around the system.
        </p>
        <br />

        <h3>1.3 An Outage at NapTime</h3>
        <p>
          What happens when an outage is reported? Say, for example, a couple of users reach out to say that they’re
          trying to access their homepages but they’re just receiving a 500-level error.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/debugging-small.gif" class="case-study-image" />
          <figcaption>Fig. 2: How can NapTime determine what's wrong?</figcaption>
        </figure>
        <br />
        <p>
          The engineers decide on three priority debugging steps: ensure that each server in their system is up, ensure
          that each of the services running on those servers is up, and check whether those services are sending and
          receiving requests and responses successfully.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/debugging-steps.gif" class="case-study-image" />
          <figcaption>Fig. 3: Some potential debugging steps for NapTime's engineers</figcaption>
        </figure>
        <br />
        <p>
          First, if they have a list of their static IP addresses, they can use the Unix command line utility `ping` to
          check whether each of the servers is up at all. If the servers are up, it’ll be useful to see their health
          metrics. NapTime engineers might run a utility like `top` on each of the servers to see a summary of the
          system’s health (e.g. overall CPU load or available memory) as well as what processes are running on the
          server.
        </p>
        <br />
        <p>
          If everything checks out with the system’s health, it’s time to start looking into the individual services
          running on each server. If the engineers are interested in a running process, like an application, they might
          check the status of their process manager. Each service generally has a different method of checking its
          health.
        </p>
        <br />
        <p>
          After the most obvious checks (e.g. is something even running?) come the next steps. What if none of their
          checks turn up an issue? Performing each of the aforementioned steps manually will tell them how something is
          performing at the moment of observation, but what about performance just before they checked? Or how about at
          the time the complaints started coming in? The log files that each of NapTime’s servers and services produce
          can provide the answers to these questions.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/logs-historical.png" class="case-study-image" />
          <figcaption>Fig. 4: NapTime's servers and services write their historical records to log files</figcaption>
        </figure>
        <br />
        <p>
          Let’s say that the NapTime team is curious about what’s going on with their Nginx web server. One of their
          next steps would be to take a look at the access log file in the nginx subdirectory of var/log.
        </p>
        <br />
        <p>
          Web servers like nginx and Apache produce access logs, where each line contains information about each request
          sent to the system; like the IP address of the requester, time of the request, the path of the requested file,
          the server response code, the time it took the server to fulfill the request, and more. If there are 500-level
          errors occurring, exploring the access log of their web server is where NapTime would find evidence of them.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/log-breakdown.png" class="case-study-image" />
          <figcaption>Fig. 5: An example of a single record from a web server's plain-text access log</figcaption>
        </figure>
        <br />
        <p>
          The individual log lines NapTime would find would look something like the above diagram. An interested
          engineer can take logs like these and analyze them, either one by one or aggregated, in order to achieve a
          number of insights, like:
          <!-- TODO: Bullet points -->
          · Finding individual specific events in the past
          · Graphing trends over time, like 500 response codes or aggregated requests per minute
          · And alerting specific users when pre-defined heuristic thresholds are met or surpassed
        </p>
        <br />

        <h3>1.4 Scaling up NapTime</h3>
        <p>
          Let’s consider NapTime’s architecture and processes after they scale a bit more. They still have a three-tier
          architecture, with presentation, business logic, and storage abstracted away from each other, but now they
          have more overall servers and services.
        </p>
        <br />
        <p>
          Collecting and processing logs is straightforward when everything is on a single machine, you just use some
          software to handle aggregation. But when you have a distributed system with multiple nodes, instead of just
          software you need both software AND some infrastructure that can handle both transportation to and storage in
          a new central location.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/naptime-scaled.png" class="case-study-image" />
          <figcaption>Fig. 6: NapTime's architecture after scaling</figcaption>
        </figure>
        <br />
        <p>
          Suddenly NapTime is in charge of 8 nodes instead of 2, scaled out app servers, more unique services, a load
          balancer, and multiple databases. What is debugging like for them now? What if MongoDB suddenly stops
          responding, or one of the app servers can no longer write to the Main Postgres instance? How will NapTime
          isolate what went wrong, let alone actually fix it? Their number of servers and services keeps growing, and
          the number of connections between all of them continues to grow exponentially. How is the team supposed to get
          a holistic understanding of what’s happening across the entire topology of their system?
        </p>
        <br />
        <p>
          Monitoring is toil. That is, if you don’t have an automated framework taking care of it for you. As detailed
          above, one key component in “monitoring” is examining the log output of your systems. That can mean reading
          through files thousands of lines long, like NapTime’s Nginx Access Log, one line at a time. No one wants to
          read log files line by line if it can be avoided. After all, one of the main motivations of the field of
          software engineering is automating the toil out of processes.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/tail.gif" class="case-study-image" />
          <figcaption>Fig. 7: Running `tail -f` on a server/service's logfile can help you understand what that server/service is doing over time</figcaption>
        </figure>
        <br />
        <p>
          On Unix-like systems, a common strategy to gain a quick insight into what’s happening with a service is to run
          the command `tail -f` on the log file to which that service writes. Tail prints the last 10 lines of a given
          file to STDOUT, and the `-f` flag indicates that the program should continue to “follow”, streaming new lines
          to STDOUT as they are written to the file.
        </p>
        <br />
        <p>
          This process worked fine for NapTime when they wanted to watch the real time stream of log messages on one
          machine or two. But what about now that their system is distributed across nearly a dozen servers? Are their
          engineers going to open a dozen terminal windows and SSH onto each individual server?
        </p>
        <br />
        <p>
          Monitoring distributed applications is exponentially harder. The Naptime engineers simply wouldn’t be able to
          watch and comprehend the stream of logs coming in for every service on each server in real time. They would
          want to be able to capture those logs as they’re written or emitted and save them for later. Ideally on a
          central server of some kind, so they don’t have to go SSHing around to find what they need in the stress of an
          outage.
        </p>
        <br />

        <h3>1.5 Observability</h3>
        <p>
          The visibility that a team like NapTime seeks into their system is summed up by the concept of observability.
        </p>
        <br />
        <!-- TODO: Ø Cindy blockquote formatting here -->
        <p>
          Due to the nature of software, no complex system is ever healthy.
        </p>
        <br />
        <p>
          Distributed systems in particular are pathologically unpredictable. When multiple services in a distributed
          system are communicating with each other over the wire, each hosted on a potentially ephemeral node, the
          possible failure conditions are nearly endless. What if a server is down? What if a service is down? What if
          both services are up but are failing to communicate for some reason? Et cetera.
        </p>
        <br />
        <p>
          In light of these challenges, a team must also keep in mind to design their systems to facilitate debugging..
          Things will break and outages will occur. Hope is not a strategy; but preparedness is.
        </p>
        <br />

        <h3>1.6 Need for a monitoring strategy</h3>
        <p>
          Things can get messy very quickly. Without a centralized platform for handling the collection, transportation,
          and storage of your data, those valuable records can get lost or never collected in the first place.
          Developing a strategy for the collection, centralization, and retention of your log and metric data means that
          you don’t prematurely forfeit the benefits that logs and metrics provide.
        </p>
        <br />
        <p>
          Dendro acts as that strategy.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/single-source.gif" class="case-study-image" />
          <figcaption>Fig. 8: Dendro provides a single source of historical truth for application state</figcaption>
        </figure>
        <br />
        <p>
          A team can SSH into each of the nodes in their system, configure and deploy a logging pipeline using Dendro’s
          cli tool, , and start shipping all of their log and metric data off of their servers and onto a central store.
          The logs and metrics produced can come either from services running on the server, like nginx or Postgres, or
          from the server itself, in the form of host metrics like CPU load or available memory.
        </p>
        <br />
        <p>
          Dendro provides a single, centralized log management solution, untangling the web of data producers and
          consumers. No more routing logs from individual sources to multiple destinations over time. No more custom
          aggregation scripts, cron jobs, or any of that. Dendro collects the logs and metrics emitted from distributed
          servers and services over time, building a single source of historical state.
        </p>
        <br />
        <p>
          That single source of truth about system and service state and performance empowers the user to:
          <!-- TODO: Bullet points -->
          · Ensure that all output or generated data is reliably and automatically captured, without having to do it
          manually
          · Avoid context switching, or jumping between databases and machines trying to dig up whatever went wrong
          · And reduce debugging time, because everything exists in a single database and can be queried and explored
          like normal relational data.
        </p>
        <br />
        <p>
          Each of these benefits brings the user closer to the holy grail for engineers: automating away manual toil.
        </p>
        <br />

        <!-- Section 2 -->
        <h2 class="h2">2 Overview of Dendro</h2>
        <h3>2.1 How does Dendro help teams?</h3>
        <p>
          To explore exactly how Dendro empowers small teams working on distributed applications, let’s think back to
          the example of NapTime after they’ve scaled their architecture.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/dendro-agent.gif" class="case-study-image" />
          <figcaption>Fig. 9: Dendro's CLI helps you install and configure a collection agent on your nodes</figcaption>
        </figure>
        <br />
        <p>
          Dendro includes a collection agent that the team installs on each node in their system. That collection agent
          gathers log and metric data from both the services running on the server, as well as information about the
          server itself.
        </p>
        <br />
        <p>
          But how does the data make the jump from those machines to the central database? That would require an
          infrastructure pipeline, in addition to the deployment of some type of collection software.
        </p>
        <br />

        <h3>2.2 What’s in Dendro?</h3>
        <p>
          This is an overview of Dendro’s architecture and the infrastructure behind this framework. We’ll dive into it
          in more detail later, but for now let’s stick to the main conceptual pieces.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/dendro-architecture.gif" class="case-study-image" />
          <figcaption>Fig. 10: An overview of Dendro's architecture, split into conceptual components</figcaption>
        </figure>
        <br />
        <p>
          We’ve already discussed how Dendro helps you set up the automated collection of logs and metrics from your
          services. You don’t need to change a single line of your application code-- Dendro runs as a fully-decoupled
          agent process on your nodes, tapping into the log files to which your services already write and scraping
          metrics from the server. But collecting your logs and metrics is only the tip of the iceberg.
        </p>
        <br />
        <p>
          The collection agent streams data at the time of generation to your own pipeline that Dendro creates. Getting
          the data off of the machine as quickly as possible is important for both your ability to monitor in real-time,
          as well as ensuring that data isn’t lost if an ephemeral virtual machine gets spun down outside of your
          control.
        </p>
        <br />
        <p>
          The pipeline also features a processing step where the collectors deployed by Dendro take the raw data coming
          off of your servers and rework it into predictable, structured output, before storing it in a time-series
          database optimized for writing, processing, and querying time-series data. Dendro builds a single source of
          truth for you with each table representing a different capture source, while preserving the identity of the
          host machine or process for each row of data.
        </p>
        <br />
        <p>
          All of your monitoring data existing in the same place (your Timestream database) allows you to keep context
          switching at an absolute minimum while debugging, while also maintaining high-resolution in your data so you
          can visualize and explore granular changes in the performance and state of your servers and services over
          time.
        </p>
        <br />

        <h3>2.3 Why did we build Dendro?</h3>
        <p>
          The goal we had in mind when we set out to build Dendro was to help teams start collecting, centralizing,
          storing, and getting value out of their logs and metrics today, so they have them when they need them.
        </p>
        <br />

        <!-- Section 3 -->
        <h2 class="h2">3 Who should use Dendro?</h2>
        <p>
          When it comes to picking a monitoring solution, companies generally consider three options: buy, operate, or
          build.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/buy-operate-build.gif" class="case-study-image" />
          <figcaption>Fig. 11: Teams generally choose between buying, personally operating, or building their own monitoring solution</figcaption>
        </figure>
        <br />
        <p>
          If a small company like NapTime was to <span class="bold">buy</span> a solution, they might look at working
          with a SaaS vendor like Datadog. Outsourcing your monitoring to a vendor makes the whole process very easy and
          painless, but that ease comes with a pretty steep price. Not every small company has enough runway to justify
          spending money on yet another SaaS service.
        </p>
        <br />
        <p>
          The next option then is to <span class="bold">operate</span>. Open source platforms like Elastic were
          developed to make this relatively simple. However, if you want to use the open source Elastic Stack, you’re
          still responsible for hosting all of it, and managing an Elasticsearch server requires knowledge of Java and
          prolonged attention. What if NapTime’s engineers don’t have that expertise or time?
        </p>
        <br />
        <p>
          The third option, <span class="bold">build</span>, simply isn’t feasible for a small company. If you’re a tech
          giant like Facebook or Twitter, with a huge budget and plenty of infrastructure, you can roll your own
          observability platform. For a small company, their time is much better spent on their own core-business
          product that keeps the lights on.
        </p>
        <br />
        <p>
          But what if there were another option? That’s where we think Dendro fits in.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/dendro-comparison.png" class="case-study-image" />
          <figcaption>Fig. 12: How our team chose to position Dendro in the marketplace of monitoring solutions</figcaption>
        </figure>
        <br />
        <p>
          Dendro combines the <span class="bold">ease of deployment</span> and <span class="bold">low maintenance</span>
          of a SaaS solution thanks to its collection agents and fully-serverless pipeline.
        </p>
        <br />
        <p>
          Dendro mirrors the <span class="bold">ownership of data</span> promised by Open Source or DIY solutions. Log
          data can be extremely sensitive, and you don’t necessarily want someone else having access to your records in
          their god view/admin platform.
        </p>
        <br />
        <p>
          However, Dendro isn’t nearly as <span class="bold">feature rich</span> as any of the other solutions. We built
          Dendro with a specific, niche use-case in mind and can’t come close to offering the richness of a Datadog,
          Elastic Stack, or highly custom DIY platform.
        </p>
        <br />
        <p>
          One of Dendro’s main differentiators is that <span class="bold">Dendro treats time-series data as a
            first-class citizen</span>. The time-series database in Dendro’s pipeline is optimized for inserting and
          indexing time-based data, and comes with a whole slew of time-series query and aggregation functionality that
          Elasticsearch does not support.
        </p>
        <br />
        <p>
          But, what we traded for the ease and speed of setup and time-series optimization is cost. We designed the
          pipeline using AWS components, and while their pricing can be opaque and tricky to calculate at times Dendro
          certainly costs more to keep running than the Elastic Stack or a DIY platform because you’re not operating the
          platform on your own infrastructure.
        </p>
        <br />


        <!-- Section 4 -->
        <h2 class="h2">4 Design Decisions</h2>

        <p>
          Our primary goal while designing Dendro was to help teams start collecting, centralizing, storing, and getting
          value out of their logs and metrics as soon as possible. That way, teams have their monitoring data already
          saved and explorable when they need it the most: in the stress of an outage. Our team took that goal and broke
          it up into three components:

          <!-- TODO: Bulletpoints -->
          <span class="bold">The need for a “low-toil framework”</span>
          <span class="bold">Treating time-series data as a first class citizen</span>
          <span class="bold">Decreasing Mean Time to Resolve (MTTR)</span>
        </p>
        <br />

        <h3>4.1 Low-toil</h3>
        <p>
          A low-toil framework is one that is easy to set up and maintain, helping your core development flow without
          adding to your busy workload. There are 3 factors the team decided were crucial to determining whether a
          framework could be called “low-toil”:
        </p>
        <br />

        <h5>Ease of Integration</h5>
        <p>
          To start aggregating logs and scraping metrics, users generally need to deploy a collection tool onto their
          servers. Ideally, such a tool should be able to collect, transform and route all monitoring data. Moreover, it
          should be able to configure monitoring for multiple services without requiring users to write custom
          integration code.
        </p>
        <br />
        <h5>A ready-made pipeline</h5>
        <p>
          A low-toil framework should provision and deploy a pre-designed/opinionated logging pipeline for users,
          freeing them from the need to research and construct their own -- enabling them to focus on their core
          functionality.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/api-calls.png" class="case-study-image" />
          <figcaption>Fig. 13: Dendro's CLI commands abstract away the dozens of API calls to AWS required for setting up a pipeline</figcaption>
        </figure>
        <br />
        <h5>Low maintenance</h5>
        <p>
          Provisioning and maintaining servers takes time and can be difficult. And even under close observation, they
          can still crash at seemingly random times. The ideal low-toil framework would feature a pipeline that is
          completely serverless to avoid extra stress and overhead for development teams.
        </p>
        <br />
        <p>
          Every service in Dendro’s pipeline is fully serverless and managed for you by AWS. Users don’t need to worry
          about Dendro crashing or becoming bogged down during peak loads.
        </p>
        <br />

        <h3>4.2 Time-series native</h3>
        <p>
          Time series data is a collection of observations obtained through repeated measurements over time. Think of a
          graph that has time as the x-axis-- that’s time-series data. At the level of a single record, time-series data
          is indistinguishable from normal relational data that features a timestamp as a field.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/timestream-schema.png" class="case-study-image" />
          <figcaption>Fig. 14: An example schema for Timestream, AWS's time-series database</figcaption>
        </figure>
        <br />
        <p>
          With that in mind, log data is perfectly suited to being treated as time-series data. For example, time series
          data could measure the average response time of a web server over the course of a week. The distinction
          between normal relational data and time-series data is more meaningful when one thinks about aggregating and
          querying data at the database level.
        </p>
        <br />
        <!-- TODO: Kreps block quote: “A log is an append-only, totally-ordered sequence of records ordered by time.” -->
        <p>
          Elasticsearch & other such stores/services are optimized for text-based indexing and querying, but are not
          optimized for time, and so typical queries you’d want to perform with time-based data are either slow or not
          possible to perform.
        </p>
        <br />
        <p>
          Mapping logs from plain-text log lines to time-series records in a database unlocks a large range of
          time-specific querying functionality that helps Dendro improve your ability to extract insights from your logs
          over time.
        </p>
        <br />
        <p>
          If we look at a web server such as Apache or nginx, we can think of a few important metrics that use time as a
          measurement.
          <!-- TODO: Numbers -->
          1. Request time duration - “What happened yesterday that caused our average request time to double?”
          2. Request status codes - “What happened at 4pm that started causing 500 status codes?”
          3. Requests per second - “When are our slowest hours so the team can schedule maintenance?”
        </p>
        <br />

        <h3>4.3 Decrease Mean Time to Resolve (MTTR)</h3>
        <p>
          Getting insights into distributed systems is cumbersome.
        </p>
        <br />
        <p>
          For example, think back to NapTime’s scaled architecture. Their system includes 8 different nodes and 16
          different network connections that could fail. If some error occurs, where did it happen? What node failed?
          What logs do they check? Without log aggregation and centralization, NapTime would have to check each node
          individually.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/connections.png" class="case-study-image" />
          <figcaption>Fig. 15: How can engineers debug systems with exponentially growing numbers of nodes/connections?</figcaption>
        </figure>
        <br />
        <p>
          Dendro aims to decrease the time between an outage occurring and the time it takes for you to resolve it. We
          don’t want your outages to be a murder mystery. And we don’t want you or your team being reactive.
        </p>
        <br />
        <p>
          Instead, we want you to be proactive. Dendro not only helps you catch issues early, but also informs you in
          real-time precisely what’s broken, helping you pinpoint exactly what went wrong and on which node. Dendro does
          so by monitoring incoming logs and metrics to notify users of failures, thereby decreasing the time it takes
          for your team to become aware of a problem.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/proactive.png" class="case-study-image" />
          <figcaption>Fig. 16: Dendro helps you be proactive, rather than reactive, in debugging your system</figcaption>
        </figure>
        <br />
        <p>
          With Dendro, NapTime’s engineer’s no longer need to check 8 different nodes. By aggregating logs into a single
          database and providing a dashboard to query and view health metrics, Dendro helps solve outages efficiently
          without your engineers having to play detective.
        </p>
        <br />


        <!-- Section 5 -->
        <h2 class="h2">5 Dendro's Architecture</h2>
        <br />
        <figure>
          <img src="assets/images/case-study/architecture.png" class="case-study-image" />
          <figcaption>Fig. 17: Dendro's architecture</figcaption>
        </figure>
        <br />
        <p>
          We’ve seen how Dendro’s architecture is split up into four major conceptual sections. In this next section
          we’re going to more closely examine these pieces as well as the individual components that comprise them.
        </p>
        <br />

        <h3>5.1 Collection</h3>
        <br />
        <figure>
          <img src="assets/images/case-study/collection.png" class="case-study-image" />
          <figcaption>Fig. 18: Dendro's collection step</figcaption>
        </figure>
        <br />
        <p>
          For each service in a user’s distributed app that they want to monitor, they need to set up collection. Dendro
          uses a collection agent called <a href="https://vector.dev/" target="_blank">Vector</a> to gather, transform,
          and send a user’s distributed logs to the pipeline.
        </p>
        <br />
        <p>
          Raw logs are text based, unqueryable and generally hard to work with. Vector makes it possible to take this
          raw, plaintext nginx access log and convert it to JSON prior to sending the log off to the pipeline for
          further processing.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/highlighted-log.png" class="case-study-image" />
          <figcaption>Fig. 19: Extracting information from raw logs is difficult</figcaption>
        </figure>
        <br />
        <p>
          Looking at this log, we can see several important pieces of information. The path that was requested, the
          amount of time it took to complete the request, as well as the time the event occurred. Dendro needs to
          somehow identify and capture this information from this dynamic log.
        </p>
        <br />
        <p>
          To properly function on each node it’s deployed on, Vector requires a lengthy configuration file. It includes
          all of the log and metric sources you want to gather from and the sinks you’re sending to. It requires writing
          esoteric regexes to transform those logs from plaintext to a structured format, and configuring your AWS
          credentials to ship to the pipeline. Dendro adds a “type” field to each collected record, identifying the
          service that produced the log. This type property is crucial for a sorting step that occurs later on in the
          pipeline and which we’ll discuss below.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/vector-config.png" class="case-study-image" />
          <figcaption>Fig. 20: A basic config file for vector is dozens of lines long</figcaption>
        </figure>
        <br />
        <p>
          Writing out all of this information is extremely laborious. A fully fleshed out configuration file is over 400
          lines long, and this caused our team plenty of headaches when we were first manually typing it out.
        </p>
        <br />
        <p>
          In order to streamline this, the team built a `dendro configure` CLI command that launches a configuration
          wizard, providing a prompt to select which services our users want to monitor. With these services selected,
          Dendro is able to dynamically generate the long config file Vector requires without manual effort.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/dendro-config.gif" class="case-study-image" />
          <figcaption>Fig. 21: Dendro's configure command</figcaption>
        </figure>
        <br />
        <p>
          Once Vector is properly configured, it gathers logs and metrics, transforms them from plaintext to JSON, and
          then sends them off to the cloud pipeline. Once in the pipeline, the first stop is AWS Kinesis Data Firehose.
        </p>
        <br />

        <h3>5.2 Transform</h3>
        <br />
        <figure>
          <img src="assets/images/case-study/transform.png" class="case-study-image" />
          <figcaption>Fig. 22: Tranformation section of Dendro's cloud infrastructure</figcaption>
        </figure>
        <br />
        <p>
          Kinesis Data Firehose is a fully-managed, real-time streaming platform that captures, transforms & delivers
          data. It ingests new data up to a predefined threshold (Dendro defaults to use one minute/1MB) in order to
          optimize network efficiency, and because it’s completely serverless, it adds no additional infrastructure for
          the users of Dendro to manage.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/connections-bad.png" class="case-study-image" />
          <figcaption>Fig. 22: An impossible to manage web of connections between services</figcaption>
        </figure>
        <br />
        <p>
          In today’s distributed world, data produced by one service is often consumed by multiple other services.
          Connecting all of these is difficult, time-consuming and hyper-specific. Dendro uses Kinesis to help decouple
          services from one another. Instead of connecting every service together in a hyper-specific manner, each
          service can publish to or consume from the Data Firehose.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/connections-good.png" class="case-study-image" />
          <figcaption>Fig. 23: Services decoupled from one another</figcaption>
        </figure>
        <br />
        <p>
          Once the data buffering threshold is met, data is pointed to the static HTTP Endpoint of an S3 bucket. Data is
          temporarily staged in the bucket for two main reasons: to assist with retries of failed deliveries and persist
          any records that errored out before being stored in Timestream (which will prompt an alert from Dendro’s
          monitoring setup, as we’ll discuss later), as well as to trigger a Lambda function on being written.
        </p>
        <br />
        <p>
          Lambda is a serverless compute service that allows users to execute code without provisioning servers. Lambdas
          can be invoked in response to certain triggers making them event-driven, and they can scale proportionally to
          the incoming workload by concurrently invoking more instances of a function when needed. This ability to
          continuously scale is perfect for Dendro because log data tends to be very bursty.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/lambda-go.png" class="case-study-image" />
          <figcaption>Fig. 24: Dendro's Lambda function sorting and inserting records into Timestream</figcaption>
        </figure>
        <br />
        <p>
          The Lambda function serves to transform, sort, and load data into Timestream, our selected time-series
          database. The team initially wrote the Lambda function in JavaScript, but later rewrote it in Golang because
          the Lambda function is pivotal to the throughput of data ingestion and using a compiled language offered
          faster speeds and lower overhead. This rewrite to Go netted us a two-fold increase in throughput when
          processing 1,000 records.
        </p>
        <br />
        <p>
          Once invoked, Dendro’s Lambda function retrieves the newly written records from the bucket, sorts, and stores
          them into separate tables in Timestream based on the type property, which is added earlier using Vector.
        </p>
        <br />

        <h3>5.3 Store</h3>
        <p>
          This takes us to Timestream, a time-series optimized database service. With the user’s data extracted,
          transformed, and finally loaded into Timestream, they are able to analyze and query this data across arbitrary
          periods of time. Dendro has taken their raw logs and metrics and converted them into a form that's easily
          analyzable and processable.
        </p>
        <br />
        <p>
          This is possible because Timestream provides a SQL-like language to query this data. This query language is
          extremely powerful because not only is it already familiar to most developers, but also it adds some very
          powerful functions that work with time-series datasets.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/timeseries-query.png" class="case-study-image" />
          <figcaption>Fig. 25: Time-series analysis with SQL</figcaption>
        </figure>
        <br />
        <p>
          The power of a time-series database can be summed up by the idea that changes in data over time are tracked as
          INSERTS, rather than as UPDATES. There’s no rewriting of existing data when you’re thinking in terms of
          time-series, there are just multiple observations over time. This unlocks rich potential for graphing,
          visualizing, and understanding trends in your data over time-- precisely what the team set out to do when we
          designed Dendro.
        </p>
        <br />

        <h3>5.4 Monitor</h3>
        <br />
        <figure>
          <img src="assets/images/case-study/monitor.png" class="case-study-image" />
          <figcaption>Fig. 26: Monitoring section of Dendro's cloud infrastructure</figcaption>
        </figure>
        <br />
        <p>
          With the aforementioned infrastructure in place, the team was able to build a monitoring and alerting system
          to harness the incoming logs. This enables some useful functionality, like the ability to email our users as
          soon as critical errors in their distributed system occur (like a web server suddenly only returning 500
          response codes). This core functionality is made possible by CloudWatch.
        </p>
        <br />
        <p>
          CloudWatch provides a centralized view of all the logs coming from the user’s cloud infrastructure. Dendro
          users are able to set up metric filters on incoming logs and set heuristic thresholds that, when activated,
          will set off an alarm. Triggered alarms publish an event to their corresponding Simple Notification Service
          topic, which will in turn email any users that have subscribed to that topic during setup. This allows us to
          watch for any errors as they occur and immediately notify our users.
        </p>
        <br />
        <p>
          Finally, we arrive at Dendro’s custom-built data exploration and dashboarding server.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/dashboard-charts.png" class="case-study-image" />
          <figcaption>Fig. 27: Dendro's chart page</figcaption>
        </figure>
        <br />
        <p>
          We built a dashboarding hub to help users visualize and query their data in near real-time as it's being
          produced. This dashboard is available for view on any machine in the user’s distributed system that has Dendro
          installed. The team has pre-populated the dashboard with charts that allow users of Dendro to isolate
          anomalies and see historical trends. These charts are generated and populated in real time, providing an
          up-to-the-minute view of system state in a single location.
        </p>
        <br />
        <br />
        <figure>
          <img src="assets/images/case-study/dashboard-query.png" class="case-study-image" />
          <figcaption>Fig. 28: Dendro's query page</figcaption>
        </figure>
        <br />
        <p>
          Because accessing AWS functionality through their actual website GUI can take time and effort, we also built
          in the ability to query Timestream directly from the dashboarding hub, exploring any returned rows for
          convenience and exporting the data as JSON with the click of a button.
        </p>
        <br />
        <p>
          Now that we’ve individually examined each component of Dendro, we’re better equipped to understand Dendro as a
          whole and how to use it.
        </p>
        <br />


        <!-- Section 6 -->
        <h2 class="h2">6 Installing and Using Dendro</h2>
        <h3>6.1 Installing Dendro</h3>
        <!-- TODO: 5-steps-to-install.png -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/5-steps-to-install.png" class="case-study-image" />
          <figcaption>Fig. 29: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          Dendro is an npm package, which means the first step is to download and install it just like any other npm
          package. Make sure to use the -g flag so that the command is available throughout your local environment
        </p>
        <br />
        <p>
          The second step is to install the data collecting agent, Vector, on the local machine. Dendro provides a
          command which helps the user to install Vector given their particular package manager.
        </p>
        <br />
        <p>
          The third step is to configure Dendro for that server using the dendro configure command. This step results in
          two configuration files: 1. a file that Vector uses to monitor the selected services on that machine, 2. A
          file that Dendro uses to deploy the pipeline, including AWS credentials and database tables that match the
          selected services for that server.
        </p>
        <br />
        <!-- TODO: dendro-configure.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/dendro-configure.gif" class="case-study-image" />
          <figcaption>Fig. 30: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          Dendro now knows which services you want to monitor on your machine. The fourth step is to actually build the
          pipeline by running the command `dendro deploy`. Again, the pipeline provides a centralized location where all
          of the monitoring data from our various servers will live.
        </p>
        <br />
        <!-- TODO: dendro-deploy.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/dendro-deploy.gif" class="case-study-image" />
          <figcaption>Fig. 31: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          Running the dendro deploy command sets up the AWS infrastructure, including the IAM role, the Kinesis Firehose
          stream, the S3 bucket, the Lambda function, and the Timestream database with the correct tables.
        </p>
        <br />
        <p>
          It asks if the user’s AWS credentials have the appropriate permissions, providing a link where they can
          confirm that they do, then it asks if they want to set up alerting, which will send them an email under
          specific failure conditions.
        </p>
        <br />
        <!-- TODO: dendro-teardown.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/dendro-teardown.gif" class="case-study-image" />
          <figcaption>Fig. 32: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          We can run the teardown command to remove all deployed AWS services. It will ask us to confirm that the user
          wants to delete the resources. The teardown command is for ease of use and to ensure that there are no extra
          AWS services floating around in the user’s account.
        </p>
        <br />
        <!-- TODO: vector-run.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/vector-run.gif" class="case-study-image" />
          <figcaption>Fig. 33: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          Now that the pipeline is set up, we’re ready to send data to it. We’ll do this by running vector.
        </p>
        <br />
        <p>
          <!-- TODO: stylize code -->
          We start Vector by running the command <code>vector --config vector-config.toml</code>, passing in the
          configuration file generated from the configure command. This file ensures that we’re monitoring the
          appropriate services on this machine. Vector is now streaming logs and metrics to the AWS pipeline.
        </p>
        <br />

        <h3>6.2 Using Dendro</h3>
        <!-- TODO: frontend-home.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/frontend-home.gif" class="case-study-image" />
          <figcaption>Fig. 34: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          Now that we have data passing through to the database, we can visit the frontend, which is accessed by running
          the command dendro start:server from any computer that has dendro installed and configured, and then visiting
          localhost port 3000 on a browser.
        </p>
        <br />
        <p>
          There are three pages listed on the sidebar. A homepage, a charts page, and a query page. On the homepage, the
          user can see what services are being monitored and check that the pipeline is successfully writing records to
          the database.
        </p>
        <br />
        <!-- TODO: frontend-charts.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/frontend-charts.gif" class="case-study-image" />
          <figcaption>Fig. 35: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          <!-- TODO: link -->
          On the charts page, we see real-time data for the services currently being monitored. This includes key
          metrics from the RED method,
          [https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/]
          such as the rate of requests, the number of errors among those requests, and the duration of the requests.
        </p>
        <br />
        <!-- TODO: frontend-query.gif -->
        <!-- <br />
        <figure>
          <img src="assets/images/case-study/frontend-query.gif" class="case-study-image" />
          <figcaption>Fig. 36: CAPTION</figcaption>
        </figure>
        <br /> -->
        <p>
          The query page enables us to query the Timestream database. In this example, we query the nginx access logs
          for all 500 errors that occurred in the last 2 hours. A user can also export the data and begin to drill down
          to find the source of the problem.
        </p>
        <br />


        <!-- Section 7 -->
        <h2 class="h2">7 Implementation Challenges</h2>
        <p>
          We had to overcome 3 major technical challenges while building Dendro.

          <!-- TODO: Bullet points -->
          AWS: the optimistic Response object
          Real-time data transformations
          What does Dendro visualize?
        </p>
        <br />

        <h3>7.1 AWS: the optimistic Response object</h3>
        <p>
          One major challenge the team faced while building the <code>deploy</code> and <code>teardown</code> commands
          was related to spinning up or destroying AWS services that depend on the existence of another service in order
          to successfully deploy or that cannot be destroyed while another service depends on it. <code>deploy</code>
          fires off 37 API calls, while <code>teardown</code> executes 25.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/api-calls.png" class="case-study-image" />
          <figcaption>Fig. 37: The services spun up/destroyed by Dendro's <code>deploy</code> and <code>teardown</code> commands</figcaption>
        </figure>
        <br />
        <p>
          For example, certain services couldn’t be spun up before the requisite IAM permissions were created or
          attached, or Timestream tables couldn’t be created until the database was successfully created.
        </p>
        <br />
        <p>
          When an HTTP request is dependent on a successful response to a previous one, you expect to be able to make
          the first request, wait to receive the response, and only then send the second request.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/optimistic-response.png" class="case-study-image" />
          <figcaption>Fig. 38: Expected behavior when a second async request is dependent on a successful response to a first</figcaption>
        </figure>
        <br />
        <p>
          When using the AWS SDK, AWS returns an AWS Response object that doesn’t necessarily indicate that the service
          has been created! So when you predicate the next step on the receipt of the Response object and send the
          subsequent request, it often fails.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/long-polling.gif" class="case-study-image" />
          <figcaption>Fig. 39: Our solution to premature responses: implementing long-polling (with exponential backoff)</figcaption>
        </figure>
        <br />
        <p>
          To overcome this, we implemented long polling AWS with exponential back off. Dendro asks Amazon repeatedly
          whether the operation has been completed, increasing the time between failed polls at an exponential rate in
          order to avoid overwhelming the server or hitting a rate limit. Once the AWS Response object returns a state
          that is not <span class="bold">LOADING</span>, Dendro can then react to that state change accordingly.
        </p>
        <br />

        <h3>7.2 Transforming data in real-time</h3>
        <p>
          How does Dendro take raw, plain-text logs and prepare them for insertion to a time-series database? To achieve
          this, the team had to build in two separate data transformation steps throughout the pipeline. The first
          happens at the point of collection, while the second happens in the Lambda function triggered by Kinesis Data
          Firehose writing to the S3 bucket. We have already discussed how both work but to reiterate:
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/apache-log.png" class="case-study-image" />
          <figcaption>Fig. 40: Sample plaintext log line from a web server</figcaption>
        </figure>
        <br />
        <p>
          Logs are typically emitted as plain-text. During the log collection process, Dendro takes some steps to
          prepare it for insertion into the database. It first has to parse the record using a regex pattern written
          specifically to match this particular log format. Parsing that plain-text log with such a regex pattern gives
          us a structured JSON object. Dendro then injects a `type` key into the JSON object to note the record source
          and transport it off the host server.
        </p>
        <br />
        <p>
          The second data transformation happens in the Lambda function where Dendro takes an octet-stream, parses it,
          and then writes to the database.
        </p>
        <br />
        <p>
          Part of the appeal of the Kinesis Data Firehose is that it can accept data from any source: JSON, XML and
          numerous other formats. Firehose just accepts that data without checking or enforcing its validity.
          Unfortunately, the easiest way to accept any data is to convert it to binary and keep appending it to a binary
          blob. This is exactly what Firehose does and where it converts the JSON Dendro built while collecting records
          into an octet-stream.
        </p>
        <br />
        <p>
          We first start by converting the binary data into a string. This looks similar to an array of JSON records but
          is missing a few tokens (brackets, commas, etc.) making it unparsable/invalid JSON.
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/binary-to-json.gif" class="case-study-image" />
          <figcaption>Fig. 41: Using regex to parse a string back to JSON</figcaption>
        </figure>
        <br />
        <p>
          We can use regex to insert those missing tokens and then successfully parse the octet-stream back into JSON.
          Once the record is parsable, Dendro checks that the <code>type</code> key that was injected during the first
          data transformation phase. This key helps us identify the source of the record, what data to pluck out and
          store and what table within Timestream to insert it in.
        </p>
        <br />

        <h3>7.3 What to Visualize</h3>
        <p>
          When Naptime’s servers are down and they view the Dendro dashboard, what are the key metrics they need to see
          in order to resolve their outage? The challenge here wasn’t in writing SQL queries, but actually taking a step
          back and deciding how to represent what Dendro tracks and measures visually.
        </p>
        <br />
        <p>
          There are some main schools of thought on what metrics matter most for monitoring, named things like “The Four
          Golden Signals”, “USE”, and the “RED Method”. Tom Wilkie, a former site reliability engineer (SRE) at Google,
          created the microservice-oriented monitoring philosophy called RED. It focuses on rate, errors & duration, and
          the team found that it had the right balance of simplicity and specificity for our use-case, as well as that
          it was particularly well suited to time-series data.

          <!-- TODO: bullet points -->
          <span class="bold">“Rate”</span> refers to the number of requests per second a service is receiving.
          <span class="bold">“Errors”</span> is the number of the requests that receive an error response from the
          service.
          <span class="bold">“Duration”</span> is the length of time the service takes to complete a response to a
          request.
        </p>
        <br />
        <p>
          We found that RED matched our use case nicely, perhaps due to the fact that microservice architectures are
          typically distributed systems.
        </p>
        <br />
        <p>
          Tom Wilkie has written that “understanding the error rate, the request rate, and then some distribution of
          latency gives you a nice, consistent view of how your architecture is behaving.” That’s exactly what we wanted
          Dendro to provide: a consistent view of how your architecture is behaving over time.
        </p>
        <br />


        <!-- Section 8 -->
        <h2 class="h2">8 Future Work</h2>
        <p>
          Of course, an open source project is never fully finished. While we think Dendro is currently
          feature-complete, ready for people to use and immediately benefit from, there are certain future work items
          that would strengthen the overall framework.
        </p>
        <br />
        <ul>
          <li>
            We would like to provide our users the ability to inject more and more varied custom transforms into the
            pipeline. It’s possible as of now if they go and edit the configuration files, but that’s manual and Dendro
            is all about reducing toil.
          </li>
          <li>
            We’d also like to provide built-in support for MySQL, which Vector does not currently support as a native
            source but which they do plan to build as part of a future milestone.
          </li>
          <li>
            And finally, on the dashboard, we would like to provide syntax highlighting for SQL queries as well as more
            out of the box graphs and visualizations.
          </li>
        </ul>

        <!-- Section 9 -->
        <h2 class="h2">9 References</h2>
        <!-- TODO -->

        <!-- Section 10 -->
        <h2 class="h2">10 Team</h2>
        <!-- TODO -->

        <!-- <h2>2 Secrets</h2>
        <br>
        <h3>2.1 What is a secret?</h3>
        <p>
          A secret is something you want to keep <em>secret</em>. More
          specifically, it's a sensitive piece of data that authenticates or
          authorizes you to a system. [1] For example, a connection
          string that you pass to a database so you can authenticate a session
          and request data from it. Or an API token that you
          supply when you make a call to your cloud provider so you can read
          and write from its storage. Both of these pieces of information provide access to sensitive data, so you don't
          want them falling into the wrong
          hands.
        </p>
        <br>
        <h4>Secrets vs. sensitive information</h4>
        <p>
          You probably have other sensitive information you’d like to keep
          secret too, like PII (personally identifiable information). But
          they’re not secrets if they don’t directly grant you access to a
          system. While any sensitive information should be stored securely,
          the scope of our discussion here is limited to application secrets.
        </p>
        <br>
        <h4>Secrets vs. configuration</h4>
        <p>
          Configuration is important because it influences how your
          application operates, but not all of it is really private. Your
          application needs to know what environment to run in: should it run
          in dev, or prod? That’s a piece of configuration, but it doesn’t
          authenticate or authorize you in any way—so it’s not a secret.
        </p>
        <br />

        <h3>2.2 So what?</h3>
        <br>
        <h4>
          Secrets are the keys to your kingdom—yet they're constantly leaked
        </h4>
        <p>
          In 2019, researchers at the North Carolina State University scanned
          almost 13% of Github’s public repositories and found "not only is
          secret leakage pervasive–affecting over 100,000 repositories–but
          that thousands of new, unique secrets are leaked every day." [2] They
          noted it wasn't just inexperienced developers leaking secrets in
          hobby projects. Several large, prominent organizations were also
          leaking their secrets, including a popular website used by millions
          of college applicants in the US and a major government agency in
          Europe. In both cases, they exposed their respective AWS
          credentials.
          <a target="_blank" href="https://github.com/search?q=removed+aws+key&type=Commits">See for yourself</a>
          how common it is.
        </p>
        <br>
        <h4>Developers make honest but costly mistakes</h4>
        <p>
          According to the principle of least privilege, anyone working on
          your application should only have access only to the secrets they
          need to do their work. At the same time, if you’re on a small team,
          and you know everyone personally and trust their intentions, it’s
          tempting not to follow this principle. But sharing secrets freely is
          dangerous, because people make mistakes—over-privileging
          developers can lead to honest but costly mistakes. For example, in
          2017 DigitalOcean discovered that their "primary database had been
          deleted" and as their web press release stated: “The root cause
          of this incident was an engineer-driven configuration error. A
          process performing automated testing was misconfigured using
          production credentials.” [3]
        </p>
        <br>
        <h4>Malicious actors cause damage</h4>
        <p>
          Even if developers could be perfect, there are always bad actors out
          there, and mishandling your secrets can give attackers wrongful
          access to your secrets. In 2019, Capital One had a data breach that
          affected over 100 million individuals due to a vulnerability related
          to configuration secrets involving AWS S3 buckets. The attacker
          previously worked for AWS and was able to exploit a misconfigured
          firewall to extract files in a Capital One directory stored on AWS's
          servers. [4]
        </p>
        <br />

        <h3>2.3 Common practices</h3>
        <br>
        <h4>Encryption</h4>
        <p>
          While encrypting a secret protects it from immediate threat, it
          isn’t a complete solution. For example, in a Rails application, the
          convention is to
          <a href="https://guides.rubyonrails.org/security.html#custom-credentials">store your secrets in an encrypted
            secrets file</a>. You can store the encryption key to unlock it in another file or
          in an environment variable, but <em>that’s</em> a secret too. And a
          particularly sensitive one: anyone who has access to it (and your
          application) can read and edit <em>any</em> of your secrets. So, you
          don’t want it to leak. But if you plan to secure it via encryption
          first, you’ll just kick the can down the road. This is an important
          problem we’ll revisit later.
        </p>
        <img src="assets/images/case-study/encryption-problem.png" class="case-study-image large-image" />
        <br>
        <h4>Environment variables</h4>
        <p>
          The Twelve-Factor App methodology made popular the practice of
          storing configuration in environment variables to separate
          configuration from code. [5] Since secrets are often discussed in
          the context of configuration, it may feel natural to store your
          secrets in environment variables if you do so with your
          configuration. Environment variables aren’t <em>bad</em>, but it’s
          dangerous to depend on them to carry the weight of managing your
          secrets.
        </p>
        <br>
        <h5>Sourcing environment variables from files</h5>
        <p>
          Environment variables are often set from files. For example, in
          Node.js development, they’re set in a <code>.env</code> file and
          then the contents of that file are loaded into the application’s
          environment.
        </p>
        <img src="assets/images/case-study/dotenv-a.png" class="case-study-image" />
        <img src="assets/images/case-study/dotenv-b.png" class="case-study-image" />
        <p>
          This has the advantage that you simply need to use a different
          <code>.env</code>
          file for production versus development environments. But if you
          populate environment variables from files, you must ensure those files don’t get
          accidentally checked into a public repository or otherwise leaked.
          Plus, there's a glaring unanswered question: how are those
          <code>.env</code> files distributed, and is that done securely?
        </p>
        <br>
        <h5>Environment set as part of some other system</h5>
        <p>
          Some deployment and CI/CD tools provide a built-in way to set
          environment variables. Heroku, a popular Platform-as-a-Service,
          allows users to manually do so in a control panel. This is tedious
          and error-prone, and there is no fine-grained access control. A more
          general downside of letting your tools take care of it is how many
          different tools there are. Every time your team adopts a new one,
          developers have to stop and learn each new tool’s method for setting
          them, and each time they do, that’s a new opportunity for secrets to
          be mishandled.
        </p>
        <br>
        <h5>The leaky environment</h5>
        <p>
          Regardless of <em>how</em> environment variables are set, they're
          leaky. Environment variables are implicitly made available to all
          children processes, so they're passed to anything the application
          calls. They’re often dumped in plaintext for debugging and error
          reporting, so the secrets stored within them can easily end up in
          logs.
        </p>
        <br>
        <h3>2.4 Secrets in teams</h3>
        <br>
        <h4>Secrets get shared</h4>
        <p>
          Let's take a hypothetical team of four developers, and call them
          Alice, Bob, Charlie, and David. Suppose:
        </p>
        <ul>
          <li>Alice emails a config file containing secrets to Bob</li>
          <li>Bob Slacks a particular secret to Charlie</li>
          <li>Charlie accidentally checks it into version control</li>
          <li>
            David pulls that code and works off of it, and unintentionally
            writes code that later ends up logging that secret to a log file.
          </li>
        </ul>
        <p>
          Their secrets are getting around. If you were on that team, would
          you know where your secrets are?
        </p>
        <br>
        <p>
          Maybe our hypothetical team tries to share secrets securely. Maybe
          Alice decides to take a <em>screenshot</em> of a secret and send
          that to Bob in Slack—and maybe she even goes back into Slack and
          deletes that screenshot once the recipient has got the secret. Or
          alternatively, maybe she puts that secret in a file and locks it
          with a password, sends the locked file over Slack and sends the
          password to Bob over some other communication channel, like email.
        </p>
        <br>
        <p>
          This still doesn’t look great. Without a system in place, you have
          to get creative to share your secrets securely, and that makes them
          even harder to track.
        </p>
        <br>
        <h4>Teams change</h4>
        <p>Now suppose the following events occur:</p>
        <ul>
          <li>Alice quits</li>
          <li>Bob moves over from production to development</li>
          <li>New-hire Emily joins the team</li>
        </ul>
        <br>
        <p>
          When Alice quits, how do you ensure Alice doesn't retain her access
          to secrets? Does Bob still have production credentials? When Emily
          joins, does she have to ask around to find what secrets she needs?
        </p>
        <br>
        <p>
          Without a system in place, Emily may not get, in a complete and
          controlled manner, all the secrets she’ll need to do her work. She
          may not even know she needs some secrets until she gets to work,
          finds out she needs some, and has to hunt them down.
        </p>
        <br>
        <h4>Secrets change</h4>
        <p>
          Let's add one more type of event to the mix, which will surely
          happen much more often than personnel changes:
        </p>
        <ul>
          <li>Charlie updates an API token</li>
        </ul>
        <p>
          How do you ensure Charlie's teammates use the updated version? And
          what about applications that depend on it? The old token is invalid,
          so applications will crash if they try using it. Unless there’s some
          system for managing secrets sanely, Charlie may have to hunt down
          the people that need to know or the places where it needs to be
          updated, and hope that he got them all.
        </p>
        <br>
        <h4>The security/productivity balancing act</h4>
        <p>
          While secrets are extremely sensitive, they must be accessible to
          you and your application.
        </p>
        <img src="assets/images/case-study/tension.png" class="case-study-image" />
        <p>
          There’s a tension between security and productivity, though,
          especially when it comes to sharing secrets with other developers.
          For example, how do you decide on and maintain access levels in your
          team? Then, how do you securely distribute credentials to team
          members who need them? And if someone leaves the team, how do you
          know which secrets they've accessed that you now have to update?
        </p>
        <br>
        <p>
          It may feel convenient to simply be able to access secrets at any
          time but if you don’t have structure, process, and security around
          your secrets, you’ll lose visibility and control over them.
          Eventually, you’ll find yourself with a big headache called secret
          sprawl.
        </p>
        <br>
        <h3>2.5 The problem of Secret Sprawl</h3>
        <p>
          In all of the scenarios we just examined, we saw hints of secret
          sprawl. Secret sprawl is what you have when your secrets could be
          anywhere. Secret sprawl means your secrets are littered across your
          code, infrastructure, config, and communication channels.
        </p>
        <figure>
          <img src="assets/images/case-study/ccc-sprawl.png" class="case-study-image" />
          <figcaption>Secrets get sprawled across code, config, and communication
            channels.</figcaption>
        </figure>
        <img src="assets/images/case-study/infrastructure-sprawl.png" class="case-study-image large-image" />
        <figure>
          <figcaption>Secrets get sprawled across your infrastructure.</figcaption>
        </figure>
        <br>
        <h4>The questions you can't answer</h4>
        <p>
          Secret sprawl means you can't answer questions like these with any
          degree of confidence:
        </p>
        <ul>
          <li>Who has access to what secrets?</li>
          <li>When was a particular secret shared or used?</li>
          <li>
            If you need to change a secret, where do you have to change it?
          </li>
        </ul>
        <br>
        <br>
        <h2>3 Secrets Managers</h2>
        <br>
        <h3>3.1 Centralization</h3>
        <p>
          To prevent secret sprawl, you must have a single source of
          truth—one place where all your secrets live. Establishing this
          "single source of truth" is <em>centralization</em>. Centralization
          tames secret sprawl, and paves the way to gaining visibility and
          control around your secrets.
        </p>
        <figure>
          <img src="assets/images/case-study/ccc-fixed.gif" class="case-study-image" />
          <figcaption>Secret sprawl to centralization</figcaption>
        </figure>

        <p>
          Previously, we showed that secrets might be sprawled across your
          infrastructure. Perhaps they are even passed down service-to-service
          in your pipeline. But you want it to look more like this:
        </p>
        <p>
          <img src="assets/images/case-study/infrastructure-fixed.gif" class="case-study-image large-image" />
        </p>

        <p>
          After centralization, only your app has secrets, or rather, whatever
          service needs secrets will get only the secrets they need. You
          thereby reduce the attack surface area of your application.
        </p>
        <br />

        <h3>3.2 Encryption</h3>
        <p>
          In section 2.3, we mentioned that encryption alone isn't a complete
          solution. But when you combine it with centralization, you're well
          on the way. Your secrets should be encrypted client-side, meaning
          they never leave your device before they’re encrypted. They should
          remain encrypted in transit and at rest too, so they’re never seen
          nor persisted in plaintext. That includes encryption in
          communication channels, temporary stops, and persistent storage.
          Each step adds a layer of security, as illustrated below.
        </p>
        <img src="assets/images/case-study/encryption-best-practices.gif" class="case-study-image large-image" />

        <h3>3.3 Secrets managers</h3>
        <p>
          A secrets manager is a system that helps you securely store and
          manage your secrets. Secrets managers are inherently centralized and
          invariably use encryption in some way. Beyond that, they vary in the
          use case they are targeted for and in the features they offer.
        </p>
        <br>
        <h4>How to choose a secrets manager</h4>
        <p>
          In recent years, a number of secrets management solutions have
          popped up. There are several things to consider when you’re choosing
          a secrets manager. First, a secrets manager must keep your secrets
          <strong>safe</strong>. To do that, it should encrypt your secrets.
          Second, how does it accommodate multiple users? How does it let you
          <strong>share access</strong> safely? Third, you need to know how to
          actually use it in your applications. How do
          <strong>applications</strong> actually <strong>get secrets</strong>?
          You might have to significantly adjust your workflow depending on
          the solution you pick.
        </p>
        <br>
        <h4>How secrets managers work with your application</h4>
        <p>
          Let’s zoom in on that last question: how applications get secrets.
          And that’s done in one of two ways: either your application has to
          fetch the secrets it needs—so you have to write more application
          code—<em>or</em> your application is run in a certain context such
          that it already has the secrets it needs.
        </p>
        <ol>
          <li>
            A secrets manager might
            require you to make an API call within your code to fetch secrets
            from it, in which case it’s more of a decoupled and passive
            component.
          </li>
          <li>
            On the other hand, your application might also be run
            with the secrets it needs already available. For example, if you use
            an orchestration service, such as Puppet or Docker Swarm, there
            will likely be a built-in way of specifying secrets, which will then
            be made available in the environment your application code executes
            in.
          </li>
        </ol>
        <p>
          Another approach that works the second way is what the secrets manager
          SecretHub does. SecretHub runs your application as a child process
          and injects the secrets into the environment of that process. This
          gives SecretHub some level of control, as it can monitor the
          standard output and standard error streams of your application.
        </p>
        <br />

        <h3>3.4 Existing solutions</h3>
        <br>
        <h4>Vault by Hashicorp</h4>
        <p>
          Vault is the most popular commercial solution. It's highly flexible
          and extensible. For example, it integrates with the storage backend
          and identity provider of your choice, and it can integrate with a broad array of plugins.
          But Vault is widely
          regarded as complex, and can be overkill for many teams. Their own
          docs admit this, stating "Vault is a complex system that has many
          different pieces." [6] It is probably the best choice if you need some
          of the features that only Vault offers, and if your team or
          organization has the expertise and bandwidth to manage the Vault
          beast.
        </p>
        <br>
        <h4>Other commercial solutions</h4>
        <p>
          Although Vault is dominant, there are other players on the market.
        </p>
        <ul>
          <li>Doppler is an early YC startup that launched in late 2020 whose
            focus is making it "super easy" to manage secrets. One thing that
            may give users pause is that secrets are sent plaintext to Doppler.</li>
          <li>EnvKey has a different security model—it takes a "zero trust"
            approach and encrypts secrets client-side before they are sent over
            the network. Like Doppler, EnvKey is easy to get started with, but
            it is not as feature-rich: for example, it lacks secret versioning
            and the ability to segregate permissions on a per-project basis.</li>
          <li>SecretHub has client-side encryption and is feature-rich, but
            it is complex. SecretHub also redacts secrets from standard output and standard error,
            which helps to prevent secrets being visible in logs locally and/or
            in any logs that might be shipped off to third parties.</li>
        </ul>
        <p>
          Ultimately, all commercial solutions are third parties that you have
          to trust. Many teams prefer using open-source software for a variety
          of reasons, and when it comes to secrets management, there is one
          strong reason to: you have full control over the system.
        </p>
        <br>
        <h4>Open-source solutions</h4>
        <p>
          There are open-source solutions out there, ranging from utility-like
          tools that tend to require you do a lot to get up and running, to
          more complete solutions with UIs and built-in access control. The
          latter, though, tend to be targeted toward specific use cases. For
          example, Confidant, which was developed by Lyft in 2015, has a
          nice UI and intuitive access control, but it is Docker-centric and
          AWS-centric; it assumes you are using Docker and AWS roles for authorization. An example of a more
          utility-like tool is credstash, which, like Confidant, uses AWS under
          the hood. But it has limited functionality. For example, it doesn't offer logs or the
          ability to segregate secrets by project and environment. It also
          requires a fair bit of setup. For example, you need to have an AWS KMS key
          and your developers all need AWS credentials.
        </p>
        <br>
        <h4>AWS Secrets Manager</h4>
        <p>
          AWS Secrets Manager came out in 2018, and if you're AWS-native, it may
          be perfect for your team. However, if your team doesn't already use
          AWS services, it's not exactly a plug-and-play solution. Navigating
          the AWS ecosystem presents a steep learning curve in itself, and
          Secrets Manager does not come with access control set up for you out
          of the box.
        </p>
        <br>
        <h4>Summary of existing solutions</h4>
        <p>
          Existing solutions can be categorized in very broad strokes as
          ‘lightweight’, or ‘heavyweight’, where lightweight emphasizes ease
          of quickly getting started using it, and heavyweight emphasizes
          features.
        </p>
        <img src="assets/images/case-study/existing-solutions.png" class="case-study-image" />
        <p>
          Even in the lightweight category, there’s some diversity. For
          example Doppler emphasizes usability, while EnvKey
          emphasizes security. The heavyweight ones, such as SecretHub
          and Vault, tend to offer more features but at the cost of greater
          complexity. There are also several other open-source solutions, but
          they either have a lot of overhead or are built for a niche use
          case.
        </p>
        <br />
        <h3>3.5 A new solution</h3>
        <p>
          While the solutions above provide some helpful ways to manage
          application secrets, we found they weren’t optimal for small teams
          to hit the ground running with. Because of this, we built Haven.
        </p>
        <img src="assets/images/case-study/existing-solutions-with-haven.png" class="case-study-image" />
        <br>
        <br>
        <h2>4 Introducing Haven</h2>
        <br>
        <p>
          Haven is an open-source secrets manager built with small teams and
          ease of use in mind. It protects application secrets using best
          practices, plus it’s easy to integrate and use in your applications.
        </p>
        <br>
        <p>
          After we identified the components we’d need to build a good secrets manager, we realized
          AWS had trusted and long-standing services for some crucial
          components.
        </p>
        <br>
        <ol>
          <li>
            Since secrets need to be encrypted and decrypted with an
            encryption key and "successful key management is critical to the
            security of a cryptosystem" (<a href="https://en.wikipedia.org/wiki/Key_management">Wikipedia</a>), we opted
            to use the highly vetted AWS Key Management Service
            (KMS). This is the only AWS service Haven uses that does not
            have a free tier: KMS costs $1 per month per key.
          </li>
          <br>
          <li>
            Authentication and authorization is another crucial piece, and we
            chose AWS Identity and Access Management (IAM). Every time an
            entity makes a request to a non-public AWS resource, the request
            goes through IAM first. Using IAM as the gatekeeper for all
            storage and encryption logic meant that we could ensure only
            entities we authorized could read secrets, write secrets, and so
            on.
          </li>
          <br>
          <li>
            Since we wanted IAM to be the gatekeeper for storage, we also use
            AWS for storage, and we had several storage options to choose
            from. Haven sits in the critical path of your application being
            served, so low latency and high availability were important. (On
            the other hand, scalability was not a concern for us.) We chose
            Amazon DynamoDB because it has good documentation, high availability, and
            single-digit-milliseconds latency.
          </li>
        </ol>
        <br>
        <p>
          Although we use AWS under the hood, you don't need cloud expertise
          to productively use Haven. The Haven Admin needs an AWS
          account—that’s it.
        </p>
        <br />

        <h3>4.1 How does Haven work?</h3>
        <p>
          The architecture of a Haven instance can be split up into two
          components: the client side and the corresponding AWS infrastructure
          side. On the client side, each user—be it a Haven Admin, a
          developer or an application server—uses the Haven application to
          interact with the instance’s secrets. All of these users have Haven
          installed on their personal machines and are using Haven to interact
          with the same AWS infrastructure, albeit with varying levels of
          permissions.
        </p>
        <br />
        <img src="assets/images/case-study/architecture.png" class="case-study-image large-image">
        <br />
        <p>
          To showcase how Haven works, let’s walk through a common workflow,
          including how you set up Haven as an Admin, how you add a project to
          Haven, how you add developers to your Haven projects, and how you
          run your applications with Haven.
        </p>
        <br>
        <h4>Setting up Haven</h4>
        <p>
          The Haven Admin is the person responsible for creating every project
          and every user, assigning permissions to the users, and reviewing
          access logs.
        </p>
        <br>
        <p>
          Haven offers both a UI and CLI, which share the same Haven Core
          package under the hood. To get started with the Haven CLI, you
          install the haven-secrets-cli package from npm. After installing the
          npm package, you run <code>haven setup</code>, which assigns you as
          a Haven Admin.
        </p>
        <br>
        <p>
          During setup, Haven connects to your AWS account and provisions the
          backend resources for creating projects and their environments,
          adding users, setting permissions, and adding and updating secrets.
          Note that your AWS account is the <em>only</em> place your secrets
          will ever be stored with Haven—there is no external Haven server
          with your secrets.
        </p>
        <img src="assets/images/case-study/admin-setup.png" class="case-study-image large-image" />
        <p>
          Haven provisions a file called havenAccountFile, which contains your
          Haven credentials. All you need to do is place this havenAccountFile
          in your home directory.
        </p>
        <br>
        <h5>Integrating your projects</h5>
        <p>
          Let’s say you have an application called BlueJay that you want to
          integrate with Haven. Note that as the Haven Admin, you are the only
          person who will be able to create or delete projects. After you run
          <code>haven createProject BlueJay</code>, Haven provisions a
          DynamoDB table and set of IAM permission groups for your application
          BlueJay.
        </p>
        <img src="assets/images/case-study/admin-create-bluejay.png" class="case-study-image large-image" />
        <p>Next, you add all your secrets for the BlueJay project.</p>
        <img src="assets/images/case-study/admin-put-bluejay.png" class="case-study-image large-image" />
        <p>Your project BlueJay is now integrated with Haven.</p>
        <p></p>
        <br>
        <h4>Adding developers to Haven projects</h4>
        <img src="assets/images/case-study/admin-create-user.png" class="case-study-image large-image" />
        <p>
          When you create a user, Haven provisions temporary user credentials.
          They’re saved to your computer, and you’ll then send this file to
          the intended user.
        </p>
        <img src="assets/images/case-study/admin-temp-credentials.png" class="case-study-image large-image" />
        <p>
          Each developer has to install the haven-secrets-cli package from npm
          on their personal computer. Let’s switch over to the new user’s
          point of view, where we see that they’ve received the temporary
          credentials.
        </p>
        <img src="assets/images/case-study/dev-temp-credentials.png" class="case-study-image large-image" />
        <p>
          The developer puts this file in their home directory. Note that
          Haven users other than the admin don't need an AWS account since
          they'll be connecting to the Haven Admin’s AWS account. Initially,
          the developer can’t interact with any projects and secrets. The
          developer must run <code>haven userSetup</code> on their computer
          after placing their havenAccountFile in their home directory. Haven
          will then fetch their permanent credentials.
        </p>
        <img src="assets/images/case-study/dev-perm-credentials.png" class="case-study-image large-image" />
        <p>
          They are now able to start interacting with Haven based on the
          permissions you give to them. You’re able to grant them read and/or
          write permissions for secrets on a per-project, per-environment
          basis. (Granting permissions is an admin-only capability.) Depending
          on their permissions, they are able to create, update, and/or read
          secrets, and also run the application locally using
          <code>haven run</code>. Below, we depict the developer being able to
          fetch a secret that you, the admin, has stored.
        </p>
        <img src="assets/images/case-study/dev-get.png" class="case-study-image large-image" />
        <p>
          If the dev is not authorized to that particular secret, they’re
          denied access:
        </p>
        <img src="assets/images/case-study/dev-get-blocked.png" class="case-study-image large-image" />
        <br>
        <h4>Using Haven in your application</h4>
        <p>
          Finally, let’s look at how you can use Haven in your applications.
          We assume you run your application on a server, or rather, in an
          environment that has a filesystem. The Haven Admin creates a “server
          user” under this AWS account, which is really just another Haven
          user like developers are. As we saw in the previous section, the
          Haven Admin receives a havenAccountFile for the new server user. The
          Haven Admin SSHs into the server where your application is run and
          installs Haven globally or as a dependency in your project, as well
          as place the havenAccountFile in the home directory of the
          operating-system user that the application will run from. Then, the
          Haven Admin will run <code>haven userSetup</code>, just as the
          developer did in the previous section.
        </p>
        <br>
        <p>
          This server is now able to start interacting with Haven based on the
          permissions you give it. You can run your application with
          <code>haven run</code>.
        </p>
        <img src="assets/images/cli.gif" class="case-study-image" />
        <br>
        <br>
        <h2>5 Building Haven</h2>
        <br>
        <p>
          In section 3.2, we noted that there are three questions you should
          ask about any secrets manager. The decisions we made and the
          challenges we faced in building Haven can be described pretty well
          by answering those questions:
        </p>
        <ul>
          <li>How does it keep your secrets <strong>safe</strong>?</li>
          <li>How does it let you <strong>share access</strong> safely?</li>
          <li>
            How do <strong>applications</strong> actually
            <strong>get secrets</strong>?
          </li>
        </ul>
        <br />

        <h3>5.1 Keeping your secrets safe</h3>
        <br>
        <h4>Solving the “master key” problem</h4>
        <p>
          The very notion of encrypting your secrets has an inherent problem:
          what do you do with the encryption key? Assume you use symmetric
          encryption, so the encryption key both encrypts and decrypts your
          secrets. But then that encryption key is itself a secret—and a
          particularly sensitive one, since it can unlock all of your secrets.
          You might try to encrypt that key with another key, but that would
          be yet another key that you have to encrypt.
        </p>
        <img src="assets/images/case-study/encryption-problem.png" class="case-study-image large-image" />
        <p>
          One way to solve this problem is to have a trusted third-party
          service store an encryption key that you don’t have physical access
          to. Instead, you dictate who has permissions to use it to perform
          encryption and decryption operations. For Haven, that trusted
          third-party service is the battle-tested AWS Key Management Service
          (KMS). We use KMS to store this key, which we’ll call a “master
          key”, and limit encryption/decryption access to it via AWS IAM
          policies. We don’t need to worry about safely storing this master
          key since AWS handles that. Now let’s see why “master key” is an
          appropriate name (hint: it decrypts <em>other</em> encryption keys).
        </p>
        <br>
        <h4>Key wrapping</h4>
        <p>
          Key wrapping is an encryption best practice and refers to the
          technique of using two or more layers of keys to protect your data.
          It involves generating a unique data encryption key for each secret
          and encrypting the secret using that encryption key. Then the data
          encryption key is encrypted by the master encryption key. The
          encrypted key and encrypted secret are then stored until decrypted
          later. To decrypt your data, you perform this process in reverse:
          decrypting the data encryption key with the master key and then
          using the data encryption key to decrypt your secret data. Key
          wrapping is also sometimes called envelope encryption.
        </p>
        <img src="assets/images/case-study/envelope-encryption.png" class="case-study-image large-image" />
        <p>
          Key wrapping has two advantages: first, it’s harder to brute force
          the encrypted data since each is encrypted using a different key;
          second, you reduce the attack surface area, because the master key
          never sees your plaintext data—only plaintext data encryption
          keys—so an attacker would need access to both your secrets storage
          and the master key (and in addition, there’s one less instance of
          your plaintext secrets traveling along the wire). You may be
          wondering what you do with these encrypted data encryption keys: you
          store them alongside the encrypted secret itself, often in the same
          database row.
        </p>
        <br>
        <h4>Implementing encryption best practices</h4>
        <p>
          It's a common saying in software that you shouldn't "roll your own
          crypto"—you should use a vetted cryptographic library. We use the
          AWS Encryption SDK, a client-side encryption library, because it
          adheres to cryptography best practices like key wrapping. The AWS
          Encryption SDK requires a master key, so Haven uses the master key
          in AWS KMS that it creates for you in initial setup.
        </p>
        <br>
        <p>
          Haven
          follows the best practices of encrypting your secrets client-side,
          in transit, and at rest. When you add or update a secret, it's first encrypted on the client
          using the SDK and then sent encrypted in transit via TLS to be
          stored on Amazon DynamoDB, where it is encrypted at rest.
        </p>
        <br>
        <h4>Our encryption scheme as a whole</h4>
        <img src="assets/images/case-study/haven-envelope-encryption.png" class="case-study-image large-image" />
        <p>
          The diagram above shows Haven’s encryption scheme from start to
          finish. First, to encrypt a datum, a unique data encryption key is
          generated and is used to encrypt the secret on the client side as
          seen in the top right. Then, as shown in the top left, that data
          encryption key is encrypted using the singular master key stored in
          KMS. Both of these encrypted pieces of information are encrypted in
          transit via TLS and sent to DynamoDB to be stored alongside each
          other as shown in the bottom of the diagram. Thus we can see that
          Haven encrypts your data client side, in transit and at rest up on
          DynamoDB.
        </p>
        <br>
        <h4>Storing and fetching secrets</h4>
        <p>
          As we see below, Haven first makes a request to the AWS encryption
          SDK library to encrypt a secret. The SDK checks that the caller has
          the IAM permission to encrypt, and if so, generates a data
          encryption key, encrypts the secret value with it, and then encrypts
          the data encryption key with the master key. Then, Haven takes this
          encrypted data, and (if the user has permission) stores it in the
          database. Haven also stores the secret’s name, version number and
          whether the secret is flagged, in that same row.
        </p>
        <img src="assets/images/case-study/put-secret.png" class="case-study-image large-image" />
        <p>
          When you fetch a secret, Haven first fetches the encrypted secret
          from the database, then decrypts it using the Encryption SDK.
        </p>
        <img src="assets/images/case-study/get-secret.png" class="case-study-image large-image" />
        <br>
        <h4>Running the UI web application on localhost</h4>
        <img src="assets/images/case-study/localhost-ui.png" class="case-study-image large-image" />
        <p>
          We chose to run the Admin/Developer UI Dashboard from localhost in order to avoid the security issues that any
          application running on the public web faces. [7] We were inspired by EnvKey,
          whose FAQ states:
        </p>
        <blockquote>
          Unfortunately, it's still not possible to implement true
          zero-knowledge end-to-end encryption on the web. Apart from a
          fundamental chicken-and-egg problem when it comes to server trust,
          there's no way to protect against all those ever-so-convenient
          browser extensions that so many folks have given full-page
          permissions. [8]
        </blockquote>
        <p>
          A second reason we run the UI app locally is to make it clear that
          Haven does not have a backend “Haven” server, so we could not snoop
          on your secrets even if we wanted to.
        </p>
        <br>
        <h3>5.2 Sharing access safely</h3>
        <br>
        <h4>Supporting multi-project teams with fine-grained permissions</h4>
        <p>
          Enforcing the principle of least privilege is important, and Haven
          makes this easy by limiting access along three dimensions: by
          project, environment, and by action, where an action is read-only or
          read-write.
        </p>
        <img src="./assets/images/case-study/fine-grained-permissions.png" class="case-study-image large-image" />
        <p>
          Above, we see Sue has read-write access to secrets for project
          BlueJay in the Dev environment.
        </p>
        <br>
        <h4>Mitigating the "initial credentials" problem</h4>
        <p>
          Creating credentials for a new Haven user means creating a new
          <em>secret</em>. After all, those Haven credentials may permit the
          ability to read and write secrets! So, how can we ensure that we
          don't cause our <em>own</em> secret sprawl?
        </p>
        <br>
        <h5>Temporary credentials</h5>
        <p>
          Our solution was to create temporary credentials good for only one
          hour. The credentials don’t have permissions to do anything except
          request permanent credentials, so the user must ‘change their
          password’ before they can do anything else. We use an AWS Lambda
          function to enforce the one-hour limit. If someone doesn’t use their
          temporary credentials within an hour, the Haven Admin will need to
          create a new user.
        </p>
        <br>
        <p>
          The flow is illustrated below: first, the Haven Admin adds a user,
          either in the UI or the CLI, then Haven downloads a file with
          temporary credentials and the Haven Admin sends this to the new
          user. Second, the new user places the Haven file in their home
          directory and runs <code>haven userSetup</code>. Haven invokes a
          lambda using those temporary credentials, the lambda checks if
          they’re still valid, and if so, returns permanent credentials which
          Haven then puts into the user’s haven file. At this point, the new
          user would need to tell the Haven Admin that they set up their
          account, so that the Haven Admin could add them to projects and
          environments.
        </p>
        <img src="assets/images/case-study/user-setup.png" class="case-study-image large-image" />
        <br>
        <h4>Revocation of permissions and flagging of secrets</h4>
        <p>
          The Haven Admin can easily revoke any permission for any user or
          even delete users. When a user's permission to some secrets is
          revoked, the secrets are automatically flagged, and the next time
          the Haven Admin or a developer uses the UI Dashboard, they will see
          a red flag next to the secret, indicating they should rotate
          (change) that secret.
        </p>
        <br />

        <h3>5.3 Getting secrets to your applications</h3>
        <p>
          Getting secrets to your application should not itself contribute to
          your secret sprawl, and it should be as easy as possible to do. We
          settled on an approach similar to what the existing secrets manager
          SecretHub does.
        </p>
        <br>
        <h4>Secrets injection</h4>
        <p>
          SecretHub is a complex secrets manager and there are many ways of
          using it, but in one way of using SecretHub with your application,
          SecretHub runs your application as a child process and injects the
          application’s secrets into that child process as environment
          variables. [9]
        </p>
        <br>
        <p>
          Haven works the same way. This has three benefits. First, your
          application’s secrets aren't stored in a file somewhere on the
          application server (so no secret sprawl in that regard). Second, due
          to the nature of child processes, Haven can redact any secrets
          leaking out on standard output or standard error (we explain how shortly). Finally,
          this approach makes it easy for the developer: they can simply
          install the Haven package and change their application’s start
          command to include <code>haven run</code>.
        </p>
        <br>
        <h4>Why environment variables?</h4>
        <p>
          There are pros and cons of using environment variables. A weakness
          of environment variables is the environment can get leaked or
          inherited: a logging or debugging tool may dump the environment, or
          a malicious child process may inherit and read your secrets. [10] But
          environment variables do have one major security advantage: they die when their
          process dies. Any environment variables you set for that application
          are <em>for that process</em> and will disappear once your
          application stops running—leaving no trace behind, unlike a
          file. Besides the security advantage, environment variables also have two major
          pragmatic advantages: 1) they’re language and OS agnostic, and 2) many developers are familiar with them.
        </p>
        <br>
        <p>
          When we surveyed existing solutions, we noticed that
          most dedicated secrets managers either permit you to store
          secrets in environment variables if you want to (e.g. Vault) or just always put
          secrets into environment variables (e.g. EnvKey). And outside of dedicated
          secrets managers, if you're using Docker's or Kubernetes's built-in
          ways of handling secrets, you'll be setting them as environment variables.
          Since centralization and encryption are arguably far more important for security than whether you use
          environment variables, and since using them is standard, we decided to use environment
          variables.
        </p>
        <br>
        <h4>Redacting secrets from an application's standard output and standard error</h4>
        <p>
          We wanted to mitigate that risk of environment variables showing up
          in logs from processes that dump the whole environment. We did so by
          spawning your application as a child process.
        </p>
        <br>
        <h5>Spawning a child process</h5>
        <p>
          A process can simply be thought of as a running program. When you
          run your application, it runs in a process. The Node.js runtime allows spinning out sub-processes called
          "child" processes, and Haven uses the spawn method from Node's built-in <code>child_process</code> library.
          When using the spawn method, you specify the
          program you want to run and that program is run as a child process.
          The standard I/O of the child process is piped to and from the parent
          process: standard input is piped in to the child from the parent, and standard output
          and standard error are piped out from the child to the parent.
        </p>
        <img src="assets/images/case-study/child-process.png" class="case-study-image" />
        <p>
          This is what allows us to provide a simple wrapper for your
          application, making for an easy-to-use secrets manager, as well as
          what lets us intercept any logging of secrets on standard output and standard error
          and redact them for extra security.
        </p>
        <br>
        <h5>How it works with your application</h5>
        <p>
          So how does this child process technique fit into the bigger
          picture? In the example shown below, note that <code>todos</code> is the
          Haven project, <code>prod</code> is the environment and <code>nodemon todos.js</code> is
          the command that is run by Haven.
        </p>
        <img src="assets/images/case-study/redaction.gif" class="case-study-image" />
        <p>
          Haven fetches the secrets for the project/environment combination,
          then spawns a child process via the command you passed to Haven
          using the spawn method from the Node child process library. The
          secrets are injected into this child process as environment
          variables, making them available for the application. Then, as the
          application runs, Haven intercepts both standard output and standard error, redacts
          logged secrets, and logs the redacted result.
        </p>
        <img src="assets/images/case-study/haven-child-process.png" class="case-study-image large-image" />
        <br>
        <br>
        <h2>6 Future Work</h2>
        <br>
        <p>
          And that's Haven! We're an open-source secrets manager for small teams, with a UI and CLI for easy team
          management. For our next steps, we plan to add the ability for users to add secrets in bulk from JSON and YAML
          files. We also plan to add an email integration to make new user creation even smoother.
        </p>
        <br>
        <p>
          In addition, we plan to support the option for per-secret access controls (rather than per-project access
          controls), as well as provide direct plug-ins and integrations with select credential providers to support
          dynamic secrets (i.e., one-time use credentials).
        </p>
        <p>
          <br>
          <br>
        <h2>7 References</h2>
        <br>
        <p class="footnote">
          [1] <a href="https://www.hashicorp.com/resources/introduction-vault-whiteboard-armon-dadgar"
            class="footnote">https://www.hashicorp.com/resources/introduction-vault-whiteboard-armon-dadgar</a>
        </p>
        <p class="footnote">
          [2] <a href="https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_04B-3_Meli_paper.pdf"
            class="footnote">https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_04B-3_Meli_paper.pdf</a>
        </p>
        <p class="footnote">
          [3] <a href="https://www.digitalocean.com/blog/update-on-the-april-5th-2017-outage/"
            class="footnote">https://www.digitalocean.com/blog/update-on-the-april-5th-2017-outage/</a>
        </p>
        <p class="footnote">
          [4] <a href="https://www.capitalone.com/facts2019/" class="footnote">https://www.capitalone.com/facts2019/</a>
        </p>
        <p class="footnote">
          [5] <a href="https://12factor.net/config" class="footnote">https://12factor.net/config</a>
        </p>
        <p class="footnote">
          [6] <a href="https://www.vaultproject.io/docs/internals/architecture"
            class="footnote">https://www.vaultproject.io/docs/internals/architecture</a>
        </p>
        <p class="footnote">
          [6] <a href="https://en.wikipedia.org/wiki/Web_application_security#Security_threats"
            class="footnote">https://en.wikipedia.org/wiki/Web_application_security#Security_threats</a>
        </p>
        <p class="footnote">
          [8] <a href="https://www.envkey.com/faq/" class="footnote">https://www.envkey.com/faq/</a>
        </p>
        <p class="footnote">
          [9] <a href="https://secrethub.io/docs/guides/environment-variables/"
            class="footnote">https://secrethub.io/docs/guides/environment-variables/</a>
        </p>
        <p class="footnote">
          [10] <a href="https://www.honeybadger.io/blog/securing-environment-variables/"
            class="footnote">https://www.honeybadger.io/blog/securing-environment-variables/</a>
        </p>
        <br>
        <br>
        <h2>The Team</h2>
        <br>
        <br>
        <div class="section team-section">
          <div class="container">
            <div data-duration-in="300" data-duration-out="100" class="tabs w-tabs">
              <div data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a" style="
                    transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1)
                      rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg);
                    transform-style: preserve-3d;
                    opacity: 0;
                  " class="tabs-content w-tab-content">
                <div>
                  <div class="team-grid">
                    <div class="team-member-wrap">
                      <img src="assets/images/team/adam.jpg" loading="lazy" alt="" />
                      <div class="team-member-info">
                        <div class="team-member-name">Adam Isom</div>
                        <div class="team-member-location">Bay Area, CA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:adamisom@hey.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://adamisom.is" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://linkedin.com/in/adamisom" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/dan.jpg" loading="lazy" alt="" />
                      <div class="team-member-info">
                        <div class="team-member-name">Dan Marino</div>
                        <div class="team-member-location">
                          New York City, NY
                        </div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:danmarino1014@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://itsdanmarino.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://linkedin.com/in/daniel-marino-software-engineer" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/rebecca.jpg" loading="lazy" alt="" />
                      <div class="team-member-info">
                        <div class="team-member-name">Rebecca Nguyen</div>
                        <div class="team-member-location">Toronto, ON</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:rebecca@hellorebec.ca" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://hellorebec.ca" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://linkedin.com/in/rebeccabnnguyen" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/aram.jpg" loading="lazy" alt="" />
                      <div class="team-member-info">
                        <div class="team-member-name">Aram Podolski</div>
                        <div class="team-member-location">Denver, CO</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:aram.podolski@protonmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://arpodol.github.io" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://linkedin.com/in/arampo" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div> -->
    </article>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
      type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
      crossorigin="anonymous"></script>
    <script src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
      type="text/javascript"></script>
    <script>
      /*!
       * toc - jQuery Table of Contents Plugin
       * v0.3.2
       * http://projects.jga.me/toc/
       * copyright Greg Allen 2014
       * MIT License
      */
      !function (a) { a.fn.smoothScroller = function (b) { b = a.extend({}, a.fn.smoothScroller.defaults, b); var c = a(this); return a(b.scrollEl).animate({ scrollTop: c.offset().top - a(b.scrollEl).offset().top - b.offset }, b.speed, b.ease, function () { var a = c.attr("id"); a.length && (history.pushState ? history.pushState(null, null, "#" + a) : document.location.hash = a), c.trigger("smoothScrollerComplete") }), this }, a.fn.smoothScroller.defaults = { speed: 400, ease: "swing", scrollEl: "body,html", offset: 0 }, a("body").on("click", "[data-smoothscroller]", function (b) { b.preventDefault(); var c = a(this).attr("href"); 0 === c.indexOf("#") && a(c).smoothScroller() }) }(jQuery), function (a) { var b = {}; a.fn.toc = function (b) { var c, d = this, e = a.extend({}, jQuery.fn.toc.defaults, b), f = a(e.container), g = a(e.selectors, f), h = [], i = e.activeClass, j = function (b, c) { if (e.smoothScrolling && "function" == typeof e.smoothScrolling) { b.preventDefault(); var f = a(b.target).attr("href"); e.smoothScrolling(f, e, c) } a("li", d).removeClass(i), a(b.target).parent().addClass(i) }, k = function () { c && clearTimeout(c), c = setTimeout(function () { for (var b, c = a(window).scrollTop(), f = Number.MAX_VALUE, g = 0, j = 0, k = h.length; k > j; j++) { var l = Math.abs(h[j] - c); f > l && (g = j, f = l) } a("li", d).removeClass(i), b = a("li:eq(" + g + ")", d).addClass(i), e.onHighlight(b) }, 50) }; return e.highlightOnScroll && (a(window).bind("scroll", k), k()), this.each(function () { var b = a(this), c = a(e.listType); g.each(function (d, f) { var g = a(f); h.push(g.offset().top - e.highlightOffset); var i = e.anchorName(d, f, e.prefix); if (f.id !== i) { a("<span/>").attr("id", i).insertBefore(g) } var l = a("<a/>").text(e.headerText(d, f, g)).attr("href", "#" + i).bind("click", function (c) { a(window).unbind("scroll", k), j(c, function () { a(window).bind("scroll", k) }), b.trigger("selected", a(this).attr("href")) }), m = a("<li/>").addClass(e.itemClass(d, f, g, e.prefix)).append(l); c.append(m) }), b.html(c) }) }, jQuery.fn.toc.defaults = { container: "body", listType: "<ul/>", selectors: "h1,h2,h3", smoothScrolling: function (b, c, d) { a(b).smoothScroller({ offset: c.scrollToOffset }).on("smoothScrollerComplete", function () { d() }) }, scrollToOffset: 0, prefix: "toc", activeClass: "toc-active", onHighlight: function () { }, highlightOnScroll: !0, highlightOffset: 100, anchorName: function (c, d, e) { if (d.id.length) return d.id; var f = a(d).text().replace(/[^a-z0-9]/gi, " ").replace(/\s+/g, "-").toLowerCase(); if (b[f]) { for (var g = 2; b[f + g];)g++; f = f + "-" + g } return b[f] = !0, e + "-" + f }, headerText: function (a, b, c) { return c.text() }, itemClass: function (a, b, c, d) { return d + "-" + c[0].tagName.toLowerCase() } } }(jQuery);
    </script>
    <script>
      /* initialize */
      $('.toc').toc({
        'selectors': 'h2', //elements to use as headings
        'container': 'article', //element to find all selectors in
        'smoothScrolling': true, //enable or disable smooth scrolling on click
        'prefix': 'toc', //prefix for anchor tags and class names
        'highlightOnScroll': true, //add class to heading that is currently in focus
        'highlightOffset': 100, //offset to trigger the next headline
      });
    </script>
</body>

</html>